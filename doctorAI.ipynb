{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seqFile, labelFile):\n",
    "    train_set_x = pickle.load(open(seqFile+'.train', 'rb'))\n",
    "    valid_set_x = pickle.load(open(seqFile+'.valid', 'rb'))\n",
    "    test_set_x = pickle.load(open(seqFile+'.test', 'rb'))\n",
    "    train_set_y = pickle.load(open(labelFile+'.train', 'rb'))\n",
    "    valid_set_y = pickle.load(open(labelFile+'.valid', 'rb'))\n",
    "    test_set_y = pickle.load(open(labelFile+'.test', 'rb'))\n",
    "    \n",
    "    #print('train_set: ', len(train_set_y))\n",
    "    #print('valid_set: ', len(valid_set_y))\n",
    "    #print('test_set: ', len(test_set_y))\n",
    "    \n",
    "    def len_argsort(seq):    #sort the datasetto patient's with least to most admissions\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]), reverse=True)\n",
    "    \n",
    "    train_sorted_index = len_argsort(train_set_x)\n",
    "    train_set_x = [train_set_x[i] for i in train_sorted_index]\n",
    "    train_set_y = [train_set_y[i] for i in train_sorted_index]\n",
    "    \n",
    "    valid_sorted_index = len_argsort(valid_set_x)\n",
    "    valid_set_x = [valid_set_x[i] for i in valid_sorted_index]\n",
    "    valid_set_y = [valid_set_y[i] for i in valid_sorted_index]\n",
    "    \n",
    "    test_sorted_index = len_argsort(test_set_x)\n",
    "    test_set_x = [test_set_x[i] for i in test_sorted_index]\n",
    "    test_set_y = [test_set_y[i] for i in test_sorted_index]\n",
    "    \n",
    "    train_set = (train_set_x, train_set_y)\n",
    "    valid_set = (valid_set_x, valid_set_y)\n",
    "    test_set = (test_set_x, test_set_y)\n",
    "    \n",
    "    return train_set, valid_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_classes, embed_size= 200, n_layers=2, drop_prob=0.5):\n",
    "        \n",
    "        super(GRUNet, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers        \n",
    "        self.num_classes = num_classes   \n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        #self.emb = nn.Embedding(input_size, embed_size)\n",
    "        \n",
    "        \n",
    "        self.emb = nn.Linear(input_size, embed_size)\n",
    "        \n",
    "        \n",
    "        self.gru = nn.GRU(embed_size, hidden_dim, n_layers, bias=True, dropout=drop_prob)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.linear = nn.Linear(hidden_dim, num_classes)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, lengths, h):\n",
    "    \n",
    "\n",
    "        length = x.size(0)\n",
    "        batch_size = x.size(1)\n",
    "        input_size = x.size(2)\n",
    "        \n",
    "        x = x.view(-1, self.input_size)\n",
    "        #print('x1: ', x.size())\n",
    "        #b_size = x.size(0)\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, batch_size, self.embed_size)\n",
    "        #print('x2: ', x.size())\n",
    "        \n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=False)\n",
    "        #print('h: ', h.shape)\n",
    "        \n",
    "        out, h = self.gru(packed, h)\n",
    "        out, out_lengths = nn.utils.rnn.pad_packed_sequence(out)\n",
    "        #out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out, h\n",
    "    \n",
    "    def init_weight(self):\n",
    "        \n",
    "        self.emb.weight = torch.nn.init.uniform_(self.emb.weight, a=-0.01, b=0.01)\n",
    "        self.emb.bias = torch.nn.init.zeros_(self.emb.bias)\n",
    "        \n",
    "        self.gru.weight_ih_l0 = torch.nn.init.uniform_(self.gru.weight_ih_l0, a=-0.01, b=0.01)\n",
    "        self.gru.weight_hh_l0  = torch.nn.init.uniform_(self.gru.weight_hh_l0, a=-0.01, b=0.01)\n",
    "        self.gru.bias_ih_l0 = torch.nn.init.zeros_(self.gru.bias_ih_l0)\n",
    "        self.gru.bias_hh_l0 = torch.nn.init.zeros_(self.gru.bias_hh_l0)\n",
    "        self.gru.weight_ih_l1 = torch.nn.init.uniform_(self.gru.weight_ih_l1, a=-0.01, b=0.01)\n",
    "        self.gru.weight_hh_l1 = torch.nn.init.uniform_(self.gru.weight_hh_l1, a=-0.01, b=0.01)\n",
    "        self.gru.bias_ih_l1 = torch.nn.init.zeros_(self.gru.bias_ih_l1)\n",
    "        self.gru.bias_hh_l1 = torch.nn.init.zeros_(self.gru.bias_hh_l1)\n",
    "\n",
    "        self.linear.weight = torch.nn.init.uniform_(self.linear.weight, a=-0.01, b=0.01)\n",
    "        self.linear.bias = torch.nn.init.zeros_(self.linear.bias)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.emb.weight.data.fill_(0.01)\n",
    "        self.emb.bias.data.fill_(0.01)\n",
    "        \n",
    "        self.gru.weight_ih_l0.data.fill_(0.01)\n",
    "        self.gru.weight_hh_l0.data.fill_(0.01)\n",
    "        self.gru.bias_ih_l0.data.fill_(0.01)\n",
    "        self.gru.bias_hh_l0.data.fill_(0.01)\n",
    "        self.gru.weight_ih_l1.data.fill_(0.01)\n",
    "        self.gru.weight_hh_l1.data.fill_(0.01)\n",
    "        self.gru.bias_ih_l1.data.fill_(0.01)\n",
    "        self.gru.bias_hh_l1.data.fill_(0.01)\n",
    "\n",
    "        self.linear.weight.data.fill_(0.01)\n",
    "        self.linear.bias.data.fill_(0.01)\n",
    "        '''\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padMatrix(seqs, labels, input_size, num_classes):\n",
    "    lengths = np.array([len(seq) for seq in seqs]) - 1\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "    \n",
    "    #print('maxlen = ', maxlen)\n",
    "    \n",
    "    x = torch.zeros([maxlen, n_samples, input_size], dtype=torch.int64)\n",
    "    y = torch.zeros([maxlen, n_samples, num_classes], dtype=torch.int64)\n",
    "    \n",
    "    for idx, (seq,label) in enumerate(zip(seqs,labels)):\n",
    "        for xvec, subseq in zip(x[:,idx,:], seq[:-1]):\n",
    "            xvec[subseq] = 1\n",
    "        for yvec, subseq in zip(y[:,idx,:], label[1:]):\n",
    "            yvec[subseq] = 1\n",
    "    \n",
    "    lengths = torch.from_numpy(lengths)\n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(model, dataset, batchSize, input_size, num_classes, criterion):\n",
    "    \n",
    "    n_batches = int(np.ceil(float(len(dataset[0]))/float(batchSize)))\n",
    "    aucSum = 0.0\n",
    "    dataCount = 0.0\n",
    "    \n",
    "    for index in range(n_batches):\n",
    "        batchX = dataset[0][index*batchSize:(index+1)*batchSize]\n",
    "        batchY = dataset[1][index*batchSize:(index+1)*batchSize]\n",
    "        size = len(batchX)\n",
    "        \n",
    "        batchX, batchY, lengths = padMatrix(batchX, batchY, input_size, num_classes)\n",
    "        \n",
    "                    \n",
    "        \n",
    "        #print('size: ', size)\n",
    "        h = model.init_hidden(size)\n",
    "\n",
    "        auc, h = model(batchX.float(), lengths, h)\n",
    "        batchY = batchY.to(dtype= torch.float)\n",
    "\n",
    "        auc = criterion(auc, batchY)\n",
    "        \n",
    "        aucSum += auc * len(batchX)\n",
    "        dataCount += float(len(batchX))\n",
    "        \n",
    "    return aucSum/dataCount\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doctorAI(seqFile, labelFile, input_size, hidden_dim, num_classes, embed_size, batchSize=100, max_epochs=20, dropout_rate=0.5):\n",
    "    \n",
    "    \n",
    "    print(\"Loading Data ... \")\n",
    "    trainSet, validSet, testSet = load_data(seqFile, labelFile)\n",
    "    n_batches = int(np.ceil(float(len(trainSet[0])) / float(batchSize)))\n",
    "    train_size = len(trainSet[0])\n",
    "    \n",
    "    print(\"Done\")\n",
    "    \n",
    "    \n",
    "\n",
    "    model = GRUNet(input_size, hidden_dim, num_classes, embed_size)\n",
    "    model.init_weight()\n",
    "   \n",
    "    criterion = nn.BCELoss()\n",
    "    #learning_rate = 1.0\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), rho=0.95)   #need to know the learning rate\n",
    "   \n",
    "    \n",
    "    bestValidCrossEntropy = 1e20\n",
    "    bestValidEpoch = 0\n",
    "    testCrossEntropy = 0.0\n",
    "    \n",
    "    print(\"Optimization Start!!\")\n",
    "    \n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        iteration = 0\n",
    "        running_loss = 0\n",
    "        costVector = []\n",
    "        \n",
    "        #h = model.init_hidden(batchSize)\n",
    "        \n",
    "        #for index in range(n_batches-1):\n",
    "        for index in random.sample(range(n_batches), n_batches):\n",
    "            \n",
    "            batchX = trainSet[0][index*batchSize:(index+1)*batchSize]\n",
    "            batchY = trainSet[1][index*batchSize:(index+1)*batchSize]       \n",
    "            \n",
    "            size = len(batchX)\n",
    "            \n",
    "            batchX, batchY, lengths = padMatrix(batchX, batchY, input_size, num_classes)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            h = model.init_hidden(size)\n",
    "            \n",
    "            out, h = model(batchX.float(), lengths, h)\n",
    "            \n",
    "            #print(\"shape of out\", out.shape)\n",
    "         \n",
    "            batchY = batchY.to(dtype= torch.float)\n",
    "           \n",
    "            #print('shape of out: ', out.shape)\n",
    "            #print('shape of batchY: ', batchY.shape)\n",
    "            \n",
    "            cost = criterion(out, batchY)\n",
    "            cost.backward()\n",
    "            #for param in model.parameters():\n",
    "                #print(param.grad.data)\n",
    "                \n",
    "            #print('linear weight: ', model.linear.weight.grad)\n",
    "            #print('linear bias: ', model.linear.bias.grad)\n",
    "        \n",
    "            #print('gru first layer weight ih: ', model.gru.weight_ih_l0.grad)\n",
    "            #print('gru first layer weight hh: ',model.gru.weight_hh_l0.grad)\n",
    "            #print('gru first layer bias ih: ',model.gru.bias_ih_l0.grad)\n",
    "            #print('gru first layer bias hh: ',model.gru.bias_hh_l0.grad)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += cost.item()\n",
    "          \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                \n",
    "                #print(\"Model's state_dict:\")\n",
    "                #for param_tensor in model.state_dict():\n",
    "                    #print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "                \n",
    "                validAuc = calculate_auc(model, validSet, batchSize, input_size, num_classes, criterion)\n",
    "                print(\"Validation cross entropy:%f at epoch:%d\" % (validAuc, epoch))\n",
    "                if validAuc < bestValidCrossEntropy:\n",
    "                    bestValidCrossEntropy = validAuc\n",
    "                    bestValidEpoch = epoch\n",
    "                    \n",
    "                    testCrossEntropy = calculate_auc(model, testSet, batchSize, input_size, num_classes, criterion)\n",
    "                    \n",
    "                    print('Test Cross Entropy:%f at Epoch:%d' % (testCrossEntropy, epoch))\n",
    "                    #print(list(model.named_parameters()))\n",
    "                    \n",
    "                    torch.save(model, 'model2.'+str(epoch)+'.pth')\n",
    "                   \n",
    "        print(\"Epoch:%d, Mean_Cost:%f\" % (epoch, running_loss/n_batches))\n",
    "                    \n",
    "                \n",
    "            \n",
    "            \n",
    "    print('The Best Valid Cross Entropy:%f at epoch:%d' % (bestValidCrossEntropy, bestValidEpoch))\n",
    "    print('The Test Cross Entropy: %f' % testCrossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data ... \n",
      "Done\n",
      "Optimization Start!!\n",
      "Validation cross entropy:0.675275 at epoch:0\n",
      "Test Cross Entropy:0.662485 at Epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type GRUNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Mean_Cost:0.686529\n",
      "Validation cross entropy:0.664338 at epoch:1\n",
      "Test Cross Entropy:0.653844 at Epoch:1\n",
      "Epoch:1, Mean_Cost:0.671036\n",
      "Validation cross entropy:0.641077 at epoch:2\n",
      "Test Cross Entropy:0.637219 at Epoch:2\n",
      "Epoch:2, Mean_Cost:0.655627\n",
      "Validation cross entropy:0.627413 at epoch:3\n",
      "Test Cross Entropy:0.626445 at Epoch:3\n",
      "Epoch:3, Mean_Cost:0.638276\n",
      "Validation cross entropy:0.612136 at epoch:4\n",
      "Test Cross Entropy:0.611931 at Epoch:4\n",
      "Epoch:4, Mean_Cost:0.614402\n",
      "Validation cross entropy:0.583051 at epoch:5\n",
      "Test Cross Entropy:0.583585 at Epoch:5\n",
      "Epoch:5, Mean_Cost:0.572498\n",
      "Validation cross entropy:0.504739 at epoch:6\n",
      "Test Cross Entropy:0.506344 at Epoch:6\n",
      "Epoch:6, Mean_Cost:0.482321\n",
      "Validation cross entropy:0.385541 at epoch:7\n",
      "Test Cross Entropy:0.391745 at Epoch:7\n",
      "Epoch:7, Mean_Cost:0.216996\n",
      "Validation cross entropy:0.372949 at epoch:8\n",
      "Test Cross Entropy:0.380546 at Epoch:8\n",
      "Epoch:8, Mean_Cost:0.072514\n",
      "Validation cross entropy:0.369468 at epoch:9\n",
      "Test Cross Entropy:0.377483 at Epoch:9\n",
      "Epoch:9, Mean_Cost:0.056230\n",
      "Validation cross entropy:0.362307 at epoch:10\n",
      "Test Cross Entropy:0.371166 at Epoch:10\n",
      "Epoch:10, Mean_Cost:0.051457\n",
      "Validation cross entropy:0.356474 at epoch:11\n",
      "Test Cross Entropy:0.365628 at Epoch:11\n",
      "Epoch:11, Mean_Cost:0.049141\n",
      "Validation cross entropy:0.355344 at epoch:12\n",
      "Test Cross Entropy:0.364575 at Epoch:12\n",
      "Epoch:12, Mean_Cost:0.047871\n",
      "Validation cross entropy:0.354970 at epoch:13\n",
      "Test Cross Entropy:0.364236 at Epoch:13\n",
      "Epoch:13, Mean_Cost:0.047087\n",
      "Validation cross entropy:0.354658 at epoch:14\n",
      "Test Cross Entropy:0.363996 at Epoch:14\n",
      "Epoch:14, Mean_Cost:0.046539\n",
      "Validation cross entropy:0.354352 at epoch:15\n",
      "Test Cross Entropy:0.363724 at Epoch:15\n",
      "Epoch:15, Mean_Cost:0.046110\n",
      "Validation cross entropy:0.354169 at epoch:16\n",
      "Test Cross Entropy:0.363588 at Epoch:16\n",
      "Epoch:16, Mean_Cost:0.045799\n",
      "Validation cross entropy:0.353995 at epoch:17\n",
      "Test Cross Entropy:0.363419 at Epoch:17\n",
      "Epoch:17, Mean_Cost:0.045552\n",
      "Validation cross entropy:0.353771 at epoch:18\n",
      "Test Cross Entropy:0.363234 at Epoch:18\n",
      "Epoch:18, Mean_Cost:0.045345\n",
      "Validation cross entropy:0.353561 at epoch:19\n",
      "Test Cross Entropy:0.363093 at Epoch:19\n",
      "Epoch:19, Mean_Cost:0.045178\n",
      "The Best Valid Cross Entropy:0.353561 at epoch:19\n",
      "The Test Cross Entropy: 0.363093\n"
     ]
    }
   ],
   "source": [
    "train_doctorAI(seqFile='trial', labelFile='trial', input_size = 942, hidden_dim=2000, embed_size = 200, num_classes=942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
